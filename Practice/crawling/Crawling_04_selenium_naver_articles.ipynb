{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 네이버 기사 데이터 수집\n",
    "    - scrapy에서 selenium 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as pil\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scrapy\n",
    "import requests\n",
    "import json\n",
    "from scrapy.http import TextResponse\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in C:\\code\\Python_Folder\\02_MYSQL\\06_scrapy\\off\\naver_article\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject naver_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호는 5C51-42FB입니다.\n",
      "C:\\CODE\\PYTHON_FOLDER\\02_MYSQL\\06_SCRAPY\\OFF\\NAVER_ARTICLE\n",
      "└─naver_article\n",
      "    ├─spiders\n",
      "    │  └─__pycache__\n",
      "    └─__pycache__\n"
     ]
    }
   ],
   "source": [
    "!tree naver_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. xpath찾기\n",
    "- article links : [not()] 사용\n",
    "- article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua=UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=101'\n",
    "headers = {'user-agent' : ua}\n",
    "req = requests.get(url,headers=headers)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response= TextResponse(req.url, body=req.text, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004450522')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = response.xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt[not(@class=\"photo\")]/a/@href').extract() # 썸네일이 없는 기사는 두번째 dt가 없으므로 19개가 나온다\n",
    "len(links), links[0]  # [not(@class=\"photo\")] 클래스 photo가 없는 것을 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### selenium 사용 : xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements = driver.find_elements_by_xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt[not(@class=\"photo\")]/a')\n",
    "len(elements)\n",
    "\n",
    "#links = response.xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt[not(@class=\"photo\")]/a/@href').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_attribute(\"href\") element안에 있는 'href'주소를 속성값으로 가져와 WebDriver로 접속한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004450522')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [element.get_attribute(\"href\") for element in elements] \n",
    "len(links), links[0]                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004450522'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = links[0]\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(link, headers=headers)\n",
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('저금리에 코로나까지…이중고 시달리는 보험업계 \"수익성 제고해야\"',\n",
       " '101',\n",
       " '저금리에 신종 코로나바이러스 감염증(코로나19)까지 이중고를 겪고 있는 국내 보험산업이 시장 요구이익을 만족시키기 위해서는 현재보다 1.9배의 이익이 필요한 것으로 나타났다. 지난해 말 당기순이익과 비교해 생명보험사는 2.1배, 손해보험사는 1.5배의 이익이 더 필요하다는 분석이다. 보험연구원은 16일 오후 \\'제로금리 시대, 보험산업의 영향과 과제\\'를 주제로 온라인 세미나를 개최했다. 이번 세미나는 제로금리시대의 보험산업 수익성을 분석하고 수익성 개선을 위한 다양한 대응방안을 논의하기 위해 마련됐다. \\'보험산업의 수익성과 대응방안\\'을 주제로 발표에 나선 노건엽 보험연구원 연구위원은 \"보험회사 당기순이익은 최근 10년간(2010~2019년) 가장 낮은 수준을 나타내며 2017년 이후 하락 추세\"라며 \"수익성 지표인 자기자본이익률(ROE)은 10년 전에 비해 3분의 1수준으로 하락했다\"고 진단했다. 그는 \"2019년 기준 자본비용과 ROE 차이를 금액으로 환산하면 추가적인 시장 요구이익은 생보 3조6000억원, 손보 1조1000억원으로 총 4조7000억원\"이라고 덧붙였다. 보험산업의 이익이 건강한지를 분석하기 위해 노 연구위원은 내재가치기법(EV)을 활용해 분석했다. 내재가치는 보유계약과 자산 평가익 등을 통해 보험회사 미래이익과 금리에 대한 가치 변화를 파악할 수 있다.  노 연구위원은 \"실제 채권 처분이익이 당기순이익에서 차지하는 비중을 보면 2019년 기준 생보 62%, 손보 87%로 보험영업 손실을 투자영업 이익으로 상쇄하고 있다\"고 말했다. 이어 그는 \"일정 부분 채권 매각도 필요하겠지만 과도한 매각은 미래의 이익을 앞당겨 실현한 것으로 보험산업의 현재 이익구조가 건강하지 않음을 보여주는 단면이라고 할 수 있다\"고 설명했다. 또 일부사는 보유계약가치에서 마이너스가 발생할 정도로 금리하락에 따른 영향이 심각하므로 보유계약에 대한 관리방안이 필요하다고 지적했다. 노 연구위원은 특히 국내는 해외에 비해 국공채의 비중이 높으므로 회사채, 대체투자 등 비중 확대를 고려할 필요가 있다고 밝혔다. 실제로 유럽 보험사는 국고채 30%, 회사채 28%이나 국내는 국고채 41%(특수채 포함), 회사채 6%(금융채 포함)다. 아울러 손실이 발생하는 보유계약은 공동재보험, 계약 이전, 계약 재매입을 활용해 보유계약가치를 상승시켜야 한다고 조언했다. 공동재보험은 최근 감독제도가 개선됐으며 계약이전 및 계약재매입은 현행 제도로도 실시 가능하다. 실제 해외 사례를 보면 대만 알리안츠는 대만 차이나 라이프(China Life)에 고금리 계약을 이전했고 벨기에 생보사는 계약자에게 해지환급금의 10~30%를 프리미엄으로 제시하고 계약재매입을 진행했다. 계약 이전 후 대만 알리안츠는 영업이익이 증가했고 차이나 라이프는 지급여력(RBC) 비율은 하락했으나 영업 및 고객 기반 확대로 인해 시장점유율이 상승하는 효과를 거뒀다. 그는 \"저금리가 지속되는 유럽은 변액보험 판매가 증가하고 있어 보증옵션을 최소화한 변액보험 판매를 통해 신계약의 수익성을 높일 수 있다\"며 \"다만 상품경쟁력을 위해 다양한 보증옵션 제공시 위험관리 전문인력과 헤지 시스템 구축이 필요하다\"고 말했다. 이어 \"현재 보험산업의 이익은 적정수준보다 낮으므로 수익성 제고를 위한 다양한 노력과 건강한 수익 구조를 위한 논의가 요구된다\"고 강조했다. 차은지 한경닷컴 기자 chachacha@hankyung.com ▶  ▶  ▶    ⓒ 한국경제 &  , 무단전재 및 재배포 금지')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = response.xpath('//*[@id=\"articleTitle\"]/text()')[0].extract()\n",
    "category = response.url.split(\"sid1=\")[1].split(\"&\")[0] #split(\"sid1\") sid1을 기준으로 리스트 구분\n",
    "content = response.xpath('//*[@id=\"articleBodyContents\"]/text()').extract()\n",
    "content = \" \".join(content).strip() #strip() 문자 양끝을 정리\n",
    "title, category, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. items.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load itemts.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load naver_article/naver_article/items.py\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NaverArticleItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/items.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NaverArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    content = scrapy.Field()\n",
    "    category = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. spider.py \n",
    "- selenium 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호는 5C51-42FB입니다.\n",
      "C:\\CODE\\PYTHON_FOLDER\\02_MYSQL\\06_SCRAPY\\OFF\\NAVER_ARTICLE\\\n",
      "잘못된 경로 - \\CODE\\PYTHON_FOLDER\\02_MYSQL\\06_SCRAPY\\OFF\\NAVER_ARTICLE\\\n",
      "\n",
      "에 하위 폴더가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "!tree naver_article/ \n",
    "\n",
    "#naver_article 디렉토리에 있는 scrapy.cfg파일을 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spider실행 -> start_requests 실행 -> start_requests매서드에서 selenium 실행 \n",
    "# links 수집이되면 parse_content로 넘어간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/spiders/spider.py\n",
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from naver_article.items import NaverArticleItem \n",
    "\n",
    "# naver_article디렉토리에 있는 scrapy.cfg파일 위치를 기준으로 items를 실행하므로 from naver_article.items\n",
    "# items.py를 전역영역설정을 하면 어디서든 import 가능\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = 'NaverArticle'\n",
    "    \n",
    "    #allow_domain = ['https://news.naver.com']\n",
    "    #start_urls = ['https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=100']\n",
    "    \n",
    "    #def parse(self, response):\n",
    "        # selenium\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=101'\n",
    "        #selenium\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        elements = driver.find_elements_by_xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt[not(@class=\"photo\")]/a')\n",
    "        links = [element.get_attribute(\"href\") for element in elements] \n",
    "        \n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.parse_content)\n",
    "            \n",
    "    def parse_content(self, response):\n",
    "        item = NaverArticleItem()\n",
    "        item[\"title\"] = response.xpath('//*[@id=\"articleTitle\"]/text()')[0].extract()\n",
    "        item[\"category\"] = response.url.split(\"sid1=\")[1].split(\"&\")[0]\n",
    "        content = response.xpath('//*[@id=\"articleBodyContents\"]/text()').extract() \n",
    "        item[\"content\"] = \" \".join(content).strip()\n",
    "        item[\"link\"] = response.url\n",
    "        yield item\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/spiders/spider.py\n",
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from naver_article.items import NaverArticleItem\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = \"NaverArticle\"\n",
    "    allow_domain = [\"https://news.naver.com\"]\n",
    "    start_urls = [\"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=100\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(response.url)\n",
    "        elements = driver.find_elements_by_xpath('//*[@id=\"main_content\"]/div[2]/ul/li/dl/dt/a')\n",
    "        links = [element.get_attribute(\"href\") for element in elements]\n",
    "        driver.quit()\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.parse_content)\n",
    "            \n",
    "    def parse_content(self, response):\n",
    "        item = NaverArticleItem()\n",
    "        item[\"title\"] = response.xpath('//*[@id=\"articleTitle\"]/text()')[0].extract()\n",
    "        item[\"category\"] = response.xpath('//*[@id=\"lnb\"]/ul/li[2]/a/@href')[0].extract().split(\"sid1=\")[1]\n",
    "        content = response.xpath('//*[@id=\"articleBodyContents\"]/text()').extract()\n",
    "        item[\"content\"] = \"\".join(content).strip()\n",
    "        item[\"link\"] = response.url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. scrapy 프로젝트 실행\n",
    "    - windows 사용하는 경우에는 직접 cli 환경에서 실행\n",
    "    - scrapy crawl NaverArticle -o article.csv\n",
    "    - robots.txt 문제 > settings.py 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/code/Python_Folder/02_MYSQL/06_scrapy/off\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd naver_article\n",
    "scrapy crawl NaverArticle -o article.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "지정된 경로를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 2.4.0 - no active project\n",
      "\n",
      "Unknown command: crawl\n",
      "\n",
      "Use \"scrapy\" to see available commands\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl NaverArticle -o article.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>101</td>\n",
       "      <td>(서울=연합뉴스) 임수정 기자 = 16일 원/달러 환율이 하락해 1,100원대에 진...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>원/달러 환율, 1,100원대 진입…23개월여만에 최저치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>101</td>\n",
       "      <td>[아이뉴스24 한수연 기자] 주식회사 씨앤투스성진이 전문가용 컬러마스크인 '아에르 ...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>씨앤투스성진, 전문가용 컬러마스크 '아에르 프로' 출시</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                            content  \\\n",
       "18       101  (서울=연합뉴스) 임수정 기자 = 16일 원/달러 환율이 하락해 1,100원대에 진...   \n",
       "19       101  [아이뉴스24 한수연 기자] 주식회사 씨앤투스성진이 전문가용 컬러마스크인 '아에르 ...   \n",
       "\n",
       "                                                 link  \\\n",
       "18  https://news.naver.com/main/read.nhn?mode=LSD&...   \n",
       "19  https://news.naver.com/main/read.nhn?mode=LSD&...   \n",
       "\n",
       "                              title  \n",
       "18  원/달러 환율, 1,100원대 진입…23개월여만에 최저치  \n",
       "19   씨앤투스성진, 전문가용 컬러마스크 '아에르 프로' 출시  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\".\\\\naver_article\\\\article.csv\")\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. pilelines 사용\n",
    "- 크롤링한 데이터 mongdb에 저장하기 : pymongo 사용\n",
    "- 크롤링한 데이터에서 특정한 키워드가 있는 기사가 수집되면 slack메신저로 기사 내용과 링크 전송하기\n",
    "    - slack 메신져의 incoming webbhook은 1초에 1번 사용가능 > time.sleep(1) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
